---
title: ë¹…ë°ì´í„° - Spark SQL
category: Programmers ì¸ê³µì§€ëŠ¥ ë°ë¸Œì½”ìŠ¤
tags: [
    Big Data, SQL, Spark SQL
]
---

# Spark SQL

- SQL ì´ë€?
- SQL ì‹¤ìŠµ
- Spark SQL ì´ë€?
- Spark SQL ì‹¤ìŠµ

## SQL

- êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ”ë° ì‚¬ìš©
    - ëª¨ë“  ëŒ€ìš©ëŸ‰ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” SQL ê¸°ë°˜
        - Redshift, Snowflake, BigQuery, Hive
    - Sparkë„ SparkSQLì„ ì§€ì›
- ë°ì´í„° ë¶„ì•¼ì—ì„œ ë°˜ë“œì‹œ ìµí˜€ì•¼í•  ê¸°ë³¸ ê¸°ìˆ 

### ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤

- ëŒ€í‘œì ì¸ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤
    - ì„œë²„ í•œëŒ€ - MySQL, Postgres, Oracle, ...
        - ë¹ ë¥¸ ì‘ë‹µ ì†ë„, ìš©ëŸ‰ í•œê³„
    - ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ - Redshift, Snowflake, BigQuery, Hive, ...
        - í° ìš©ëŸ‰ì„ ì§€ì›
- ê´€ê²Œí˜• ë°ì´í„°ë² ì´ìŠ¤ëŠ” **í…Œì´ë¸”**ì´ ì¡´ì¬
    - í…Œì´ë¸” êµ¬ì¡°
        - í…Œì´ë¸”ì€ ë ˆì½”ë“œë¡œ êµ¬ì„±
        - ë ˆì½”ë“œëŠ” í•˜ë‚˜ ì´ìƒì˜ í•„ë“œë¡œ êµ¬ì„±
        - í•„ë“œ(ì»¬ëŸ¼)ëŠ” ì´ë¦„ê³¼ íƒ€ì…ìœ¼ë¡œ êµ¬ì„±

(ì˜ˆì œ) ì›¹ì„œë¹„ìŠ¤ ì‚¬ìš©ì/ì„¸ì…˜ ì •ë³´

- `ì‚¬ìš©ì ID` : ë³´í†µ ì›¹ì„œë¹„ìŠ¤ì—ì„œëŠ” ë“±ë¡ëœ ì‚¬ìš©ìë§ˆë‹¤ ìœ ì¼í•œ IDë¥¼ ë¶€ì—¬
- `ì„¸ì…˜ ID` : ì‚¬ìš©ìê°€ ì™¸ë¶€ ë§í¬ ë˜ëŠ” ì§ì ‘ ë°©ë¬¸í•´ì„œ ì˜¬ ê²½ìš° ì„¸ì…˜ì„ ìƒì„±
    - ì„¸ì…˜ì„ ë§Œë“¤ì–´ë‚¸ ì†ŒìŠ¤ë¥¼ ì±„ë„ì´ë€ ì´ë¦„ìœ¼ë¡œ ê¸°ë¡í•´ë‘  (ì‹œê°„ë„ í¬í•¨)
    - í•˜ë‚˜ì˜ ì‚¬ìš©ì ID ëŠ” ì—¬ëŸ¬ ê°œì˜ ì„¸ì…˜ IDë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ

ğŸ‘‰ ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ë°ì´í„° ë¶„ì„ê³¼ ì§€í‘œ ì„¤ì • ê°€ëŠ¥ (ë§ˆì¼€íŒ…, ì‚¬ìš©ì íŠ¸ë˜í”½ ë“±)

ìœ„ì˜ ì˜ˆì œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ë¡œ í‘œí˜„

- raw_data ë°ì´í„°ë² ì´ìŠ¤
    - `user_session_channel` í…Œì´ë¸”
        - ì»¬ëŸ¼ëª…: userId, íƒ€ì…: int
        - **ì»¬ëŸ¼ëª…: sessionId, íƒ€ì…: varchar(32)**
        - ì»¬ëŸ¼ëª…: channel, íƒ€ì…: varchar(32)
    - `session_timestamp` í…Œì´ë¸”
        - **ì»¬ëŸ¼ëª…: sessionId, íƒ€ì…: varchar(32)**
        - ì»¬ëŸ¼ëª…: ts, íƒ€ì…: timestamp

### SQL ì†Œê°œ

SQL(Structured Query Language)

: ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ” ë°ì´í„°(í…Œì´ë¸”)ë¥¼ ì§ˆì˜í•˜ëŠ” ì–¸ì–´

- ë‘ ì¢…ë¥˜ì˜ ì–¸ì–´ë¡œ êµ¬ì„±(DDL, DML)

DDL(Data Definition Language)

: í…Œì´ë¸” êµ¬ì¡° ì •ì˜ ì–¸ì–´

- CREATE TABLE
- DROP TABLE
- ALTER TABLE

DML(Data Manipulation Language)

: í…Œì´ë¸” ë°ì´í„° ì¡°ì‘ ì–¸ì–´

- SELECT FROM

    ```sql
    SELECT í•„ë“œ1, í•„ë“œ2, ...
      FROM í…Œì´ë¸”ëª…
     WHERE ì„ íƒì¡°ê±´
     ORDER BY í•„ë“œì§€ì • [ASC|DESC]
     LIMIT N;
    ```

- INSERT INTO
- UPDATE FROM
- DELETE FROM

ğŸ“Œ í…Œì´ë¸” ì¡°ì¸(JOIN)

: ë‘ê°œ ì´ìƒì˜ í…Œì´ë¸”ì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•

- INNER JOIN (êµì§‘í•©)
- LEFT OUTER JOIN
- RIGHT OUTER JOIN
- FULL OUTER JOIN (í•©ì§‘í•©)

## ğŸ–¥ï¸ SQL ì‹¤ìŠµ

colabì—ì„œ Redshift ê¸°ë°˜ SQL ì‹¤ìŠµ

- ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” (ìœ„ì˜ ì˜ˆì œì˜ í…Œì´ë¸”)
    - `raw_data.session_timestamp`
    - `raw_data.user_session_channel`

- ë¶„ì„í•  ê²ƒë“¤
    - ì›”ë³„ ì„¸ì…˜ ìˆ˜
    - ì›”ë³„ ì‚¬ìš©ì ìˆ˜ (MAU; Monthly Active User)
    - ì›”ë³„ ì±„ë„ë³„ ì‚¬ìš©ì ìˆ˜

- ì£¼ë¹„í„° SQL ì—”ì§„ ì„¤ì •

    SQL ì—”ì§„ ë¡œë“œ

    ```sql
    %load_ext sql
    ```

    ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (AWSì˜ Redshift)

    `%sql postgresql://ì‚¬ìš©ìID:íŒ¨ìŠ¤ì›Œë“œ@í˜¸ìŠ¤íŠ¸:í¬íŠ¸ë²ˆí˜¸/ì ‘ì†DB`

    ```sql
    # IDì™€ PWë¥¼ ìì‹ ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
    %sql postgresql://guest:Guest1!*@learnde.cduaw970ssvt.ap-northeast-2.redshift.amazonaws.com:5439/prod
    ```

- SELECT ì‹¤í–‰

    ì¼ë³„ ì„¸ì…˜ID ê°œìˆ˜ë¥¼ ì„¸ì…˜ID ê°œìˆ˜ì— ëŒ€í•˜ì—¬ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ 10ê°œ ì¶œë ¥

    ```sql
    %%sql

    SELECT DATE(ts) date, COUNT(sessionID)
      FROM raw_data.session_timestamp
     GROUP BY 1
     ORDER BY 2 DESC
     LIMIT 10;
    ```

    - JOIN ì¶”ê°€

        ì¼ë³„ ë°©ë¬¸ ì‚¬ìš©ì ìˆ˜

        ```sql
        # raw_data.user_session_channelê³¼ raw_data.session_timestamp í…Œì´ë¸”ì˜ ì¡°ì¸ì´ í•„ìš”
        %%sql

        SELECT DATE(st.ts) date, COUNT(usc.userID)
          FROM raw_data.session_timestamp st
          JOIN raw_data.user_session_channel usc ON st.sessionID = usc.sessionID
         GROUP BY 1
         ORDER BY 1
         LIMIT 10;
        ```

        'o' ë¥¼ í¬í•¨í•˜ëŠ” ì±„ë„ì˜ ê°œìˆ˜

        ```sql
        %%sql

        SELECT distinct channel FROM raw_data.user_session_channel
        WHERE channel ilike '%o%'
        ```

        (`distinct`ëŠ” ì¤‘ë³µ ì œì™¸, `ilike`ëŠ” ì†Œë¬¸ì ëŒ€ë¬¸ì êµ¬ë¶„ í•˜ì§€ ì•ŠìŒ)

### pandasì™€ ì—°ë™

`user_session_channel` í…Œì´ë¸” ì •ë³´ ê°€ì ¸ì˜¤ê¸°

```python
result = %sql SELECT * FROM raw_data.user_session_channel
type(result)
# sql.run.ResultSet

df = result.DataFrame()
```

```python
df.head()
# userid	sessionid	channel
# 0	779	7cdace91c487558e27ce54df7cdb299c	Instagram
# 1	230	94f192dee566b018e0acf31e1f99a2d9	Naver
# 2	369	7ed2d3454c5eea71148b11d0c25104ff	Youtube
# 3	248	f1daf122cde863010844459363cd31db	Naver
# 4	676	fd0efcca272f704a760c3b61dcc70fd0	Instagram
```

```python
df.groupby(["channel"]).size()
# channel
# Facebook     16791
# Google       16982
# Instagram    16831
# Naver        16921
# Organic      16904
# Youtube      17091
# dtype: int64

df.groupby(["channel"])["sessionid"].count()
# channel
# Facebook     16791
# Google       16982
# Instagram    16831
# Naver        16921
# Organic      16904
# Youtube      17091
# Name: sessionid, dtype: int64
```

`session_timestamp` í…Œì´ë¸” ì •ë³´ ê°€ì ¸ì˜¤ê¸°

```python
result = %sql SELECT * FROM raw_data.session_timestamp
df_st = result.DataFrame()
```

ìƒˆë¡œìš´ ì»¬ëŸ¼ ë§Œë“¤ê¸° (date)

```python
df_st['date'] = df_st['ts'].apply(lambda x: "%d-%02d-%02d" % (x.year, x.month, x.day))
```

date ì»¬ëŸ¼ë³„ë¡œ ì„¸ì…˜ ìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•˜ê³  dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬

```python
df_st.groupby(["date"])["sessionid"].count().reset_index(name='count').sort_values("date", ascending=False)
```

## Spark SQL

- SparkSQLê³¼ Spark Coreì˜ ì°¨ì´ì 
- SparkSQLì˜ ì¼ë°˜ì ì¸ ì‚¬ìš©ë²•

SparkSQL

: êµ¬ì¡°í™”ëœ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Spark ëª¨ë“ˆ

- ëŒ€í™”í˜• Spark ì…¸ ì œê³µ
- Dataframeì„ SQLë¡œ ì²˜ë¦¬ ê°€ëŠ¥
    - RDD ë°ì´í„°ë‚˜ ì™¸ë¶€ ë°ì´í„°(ìŠ¤í† ë¦¬ì§€ë‚˜ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤)ë¥¼ Dataframeìœ¼ë¡œ ë³€í™˜í•œ í›„ ì²˜ë¦¬

    ğŸ‘‰ ë°ì´í„°í”„ë ˆì„ì€ í…Œì´ë¸”ì´ ë˜ê³  sql í•¨ìˆ˜ë¥¼ ì‚¬ìš© ê°€ëŠ¥

- SparkSQL ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    - ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        - SparkSessionì˜ read í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” í˜¹ì€ SQL ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì½ì–´ì˜´
        - ğŸ“Œ Redshift ì—°ê²° ì—ì œ
            1. SparkSessionì„ ë§Œë“¤ë•Œ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì— ë§ëŠ” JDBC jar ì„ ì§€ì • (`.config` ì— ì§€ì •)
            2. SparkSessionì˜ read í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
                - ë¡œê·¸ì¸ ì •ë³´ì™€ ì½ì–´ì˜¬ í…Œì´ë¸” í˜¹ì€ SQL ì§€ì •
                - ê²°ê³¼ê°€ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¦¬í„´
            3. ë¦¬í„´ëœ ë°ì´í„°í”„ë ˆì„ì— í…Œì´ë¸” ì´ë¦„ ì§€ì •
            4. SparkSessionì˜ `sql()` í•¨ìˆ˜ ì‚¬ìš©

## ğŸ–¥ï¸ Spark SQL ì‹¤ìŠµ

 PySpark, Py4J íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
!pip install pyspark==3.0.1 py4j==0.10.9
```

Redshift ê´€ë ¨ JAR íŒŒì¼ì„ ì„¤ì¹˜

```bash
!cd /usr/local/lib/python3.6/dist-packages/pyspark/jars && wget https://s3.amazonaws.com/redshift-downloads/drivers/jdbc/1.2.20.1043/RedshiftJDBC42-no-awssdk-1.2.20.1043.jar
```

ğŸ“Œ êµ¬ê¸€ colab ì—ì„œì˜ pysparkì˜ jars ë””ë ‰í† ë¦¬ ê²½ë¡œ:

`/usr/local/lib/python3.6/dist-packages/pyspark/jars`

**Spark Session**

spark.jarsë¥¼ í†µí•´ ì•ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ **Redshift ì—°ê²°ì„ ìœ„í•œ JDBC ë“œë¼ì´ë²„**ë¥¼ ì‚¬ìš©í•¨ (`.config("spark.jars", ...)`)

```bash
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.jars", "/usr/local/lib/python3.6/dist-packages/pyspark/jars/RedshiftJDBC42-no-awssdk-1.2.20.1043.jar") \
    .getOrCreate()
```

### SparkSQL ë§›ë³´ê¸°

Pandasë¡œ csv íŒŒì¼ ë¡œë“œ

```bash
import pandas as pd

namegender_pd = pd.read_csv("https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv")

namegender_pd.head()
# name	gender
# 0	Adaleigh	F
# 1	Amryn	Unisex
# 2	Apurva	Unisex
# 3	Aryion	M
# 4	Alixia	F
```

Pandas ë°ì´í„°í”„ë ˆì„ â¡ï¸ Spark ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜

```bash
namegender_df = spark.createDataFrame(namegender_pd)

namegender_df.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- gender: string (nullable = true)

namegender_df.show()
# +----------+------+
# |      name|gender|
# +----------+------+
# |  Adaleigh|     F|
# |     Amryn|Unisex|
# |    Apurva|Unisex|
# |    Aryion|     M|
# |    Alixia|     F|
# |Alyssarose|     F|
# |    Arvell|     M|
# |     Aibel|     M|
# |   Atiyyah|     F|
# |     Adlie|     F|
# |    Anyely|     F|
# |    Aamoni|     F|
# |     Ahman|     M|
# |    Arlane|     F|
# |   Armoney|     F|
# |   Atzhiry|     F|
# | Antonette|     F|
# |   Akeelah|     F|
# | Abdikadir|     M|
# |    Arinze|     M|
# +----------+------+
# only showing top 20 rows

namegender_df.groupBy(["gender"]).count().collect()
# [Row(gender='F', count=65),
#  Row(gender='M', count=28),
#  Row(gender='Unisex', count=7)]
```

ğŸ“Œ ì°¸ê³ ë§í¬: [ğŸ”—](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)

ë°ì´í„°í”„ë ˆì„ì„ í…Œì´ë¸” ë·°ë¡œ ë§Œë“¤ì–´ì„œ SparkSQLë¡œ ì²˜ë¦¬

- createOrReplaceTempView : SparkSessionì´ ì‚´ì•„ìˆëŠ” ë™ì•ˆ ì¡´ì¬
- createGlobalTempView : Spark ë“œë¼ì´ë²„ê°€ ì‚´ì•„ìˆëŠ” ë™ì•ˆ ì¡´ì¬

```bash
namegender_df.createOrReplaceTempView("namegender")

namegender_group_df = spark.sql("SELECT gender, count(1) FROM namegender GROUP BY 1")

namegender_group_df.collect()
# [Row(gender='F', count(1)=65),
#  Row(gender='M', count(1)=28),
#  Row(gender='Unisex', count(1)=7)]
```

Redshiftì™€ ì—°ê²°í•´ì„œ í…Œì´ë¸”ë“¤ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¡œë”©

```bash
user_session_channel_df = spark.read \
    .format("jdbc") \
    .option("driver", "com.amazon.redshift.jdbc42.Driver") \
    .option("url", "jdbc:redshift://learnde.cduaw970ssvt.ap-northeast-2.redshift.amazonaws.com:5439/prod?user=guest&password=Guest1!*") \
    .option("dbtable", "raw_data.user_session_channel") \
    .load()

session_timestamp_df = spark.read \
    .format("jdbc") \
    .option("driver", "com.amazon.redshift.jdbc42.Driver") \
    .option("url", "jdbc:redshift://learnde.cduaw970ssvt.ap-northeast-2.redshift.amazonaws.com:5439/prod?user=guest&password=Guest1!*") \
    .option("dbtable", "raw_data.session_timestamp") \
    .load()
```

```bash
type(session_timestamp_df)
# pyspark.sql.dataframe.DataFrame
```

í…Œì´ë¸” ë·° ìƒì„± í›„ í•´ë‹¹ ë·°ë¥¼ ì´ìš©í•˜ì—¬ sql í•¨ìˆ˜ ì‹¤í–‰

```bash
user_session_channel_df.createOrReplaceTempView("user_session_channel")
session_timestamp_df.createOrReplaceTempView("session_timestamp")
```

```bash
channel_count_df = spark.sql("""
    SELECT channel, count(distinct userId) uniqueUsers
    FROM session_timestamp st
    JOIN user_session_channel usc ON st.sessionID = usc.sessionID
    GROUP BY 1
    ORDER BY 1
""")
```

> ìœ„ì˜ sqlë¬¸ì€ ë‹¹ì¥ ì‹¤í–‰ë˜ì§€ ì•Šê³  ì•„ë˜ `.show()` ë©”ì„œë“œë¥¼ í˜¸ì¶œë˜ë©´ ê·¸ë•Œ ì‹¤í–‰ëœë‹¤ â†’ â­Lazy Execution ë°©ì‹

```bash
channel_count_df
# DataFrame[channel: string, uniqueUsers: bigint]

channel_count_df.show()
# +---------+-----------+
# |  channel|uniqueUsers|
# +---------+-----------+
# | Facebook|        889|
# |   Google|        893|
# |Instagram|        895|
# |    Naver|        882|
# |  Organic|        895|
# |  Youtube|        889|
# +---------+-----------+
```

íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° ì¡°íšŒ SQLë¬¸ ì‹¤í–‰ (`like` ì´ìš©)
: ì±„ë„ëª…ì— 'o'ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ì±„ë„ì˜ ê°œìˆ˜ë¥¼ ì¶œë ¥

```bash
channel_with_o_count_df = spark.sql("""
    SELECT COUNT(1)
    FROM user_session_channel
    WHERE channel like '%o%'
""")

channel_with_o_count_df.collect()
# [Row(count(1)=50864)]
```
